<h2>DRL for Real: Disentangled Representation Learning for Real-world Applications</h2>

<p>Disentangled representation learning (DRL) is believed to be one of the possible ways for AI to fundamentally understand the world, which potentially helps alleviate wicked problems like hallucination issues in Large Multimodal Models (LMMs/MLLMs) and controllability issues in Generative Models, ultimately contributing to progress toward Artificial General Intelligence (AGI). Over the years, disentangled representation learning has garnered significant academic interest and research contributions, recognized for its ability to improve model robustness, interpretability, and generalizability.</p>

<p>However, the field currently faces a notable gap: the lack of comprehensive, realistic benchmarks and unified evaluation metrics that reflect real-world characteristics. This limitation has kept DRL confined mostly to synthetic toy scenarios, making it difficult to apply to more practical applications where it could have significant impact.</p>

<h3>Competition Overview</h3>

<p>To further promote the practical utility and real-world applicability of Disentangled Representation Learning, we are organizing this competition as part of the DRL for Real ICCV 2025 Workshop. The primary goal is to promote the development and evaluation of DRL methods on realistic datasets, thereby accelerating the transition of disentangled models from theoretical research to practical applications.</p>

<h3>Competition Tracks</h3>

<p>The competition is divided into two main tracks:</p>

<h4>1. Single-Factor Track</h4>

<p>This track focuses on disentangled representation learning for single-factor variations in images. The dataset includes over 10,000 images covering multiple scenes, dozens of objects, and hundreds of factors of variation.</p>

<ul>
  <li><strong>Dataset:</strong> High-quality images with controlled single-factor variations</li>
  <li><strong>Requirement:</strong> Participants must use disentangled models in this track</li>
  <li><strong>Evaluation:</strong>
    <ul>
      <li><em>Image Generation Quality (50%):</em> Measured using FID (Fr√©chet Inception Distance)</li>
      <li><em>Disentanglement Quality (50%):</em> Evaluated using Bi-directional DCI, which extends the traditional DCI metric by also measuring how perturbations in real images affect the corresponding latent variables in the encoder</li>
    </ul>
  </li>
</ul>

<h4>2. Multi-Factor Track</h4>

<p>This track is further divided into two sub-tracks, each with two leaderboards:</p>

<h5>2.1 General Image Dataset</h5>

<ul>
  <li><strong>Main Leaderboard:</strong> Open to all methods (not restricted to disentangled approaches)</li>
  <li><strong>Disentanglement Leaderboard:</strong> Only for disentangled methods</li>
  <li><strong>Evaluation for Main Leaderboard:</strong> Uses LLM+VQA evaluation approach:
    <ul>
      <li>VQA assessment of whether models correctly generate images with attributes specified in given text</li>
      <li>LLM evaluation of attribute strength in generated images</li>
      <li>Comparison between text-specified attribute changes and LLM-detected attribute changes, with scores closer to 1 indicating better performance (minimal unintended attribute changes)</li>
    </ul>
  </li>
  <li><strong>Evaluation for Disentanglement Leaderboard:</strong> Same metrics as the Single-Factor Track (FID and Bi-directional DCI)</li>
</ul>

<h5>2.2 Autonomous Driving Dataset</h5>

<ul>
  <li><strong>Dataset:</strong> Multi-camera setup with seven cameras (six surrounding the vehicle and one overhead view)</li>
  <li><strong>Task:</strong> Given images from one camera viewpoint, generate images for other viewpoints and time points</li>
  <li><strong>Main Leaderboard:</strong> Open to all methods, evaluated using SSIM to measure similarity between generated images and ground truth</li>
  <li><strong>Disentanglement Leaderboard:</strong> Only for disentangled methods, using the same metrics as the Single-Factor Track</li>
</ul>

<h3>Key Challenges</h3>

<p>This competition addresses several key challenges in the field of disentangled representation learning:</p>

<ul>
  <li><strong>Real-world Data Complexity:</strong> Moving beyond synthetic datasets to handle the complexity and ambiguity of real-world data</li>
  <li><strong>Robustness to Perturbations:</strong> Developing representations that maintain their disentangled properties under various perturbations</li>
  <li><strong>Human Interpretability:</strong> Creating representations that align with human-understandable concepts</li>
  <li><strong>Practical Utility:</strong> Demonstrating the value of disentanglement in downstream tasks and applications</li>
</ul>

<h3>Datasets</h3>

<p>Participants will have access to several datasets specifically curated for this competition:</p>

<ul>
  <li><strong>Training Datasets:</strong> Real-world data with annotated factors of variation</li>
  <li><strong>Validation Datasets:</strong> For preliminary evaluation during the development phase</li>
  <li><strong>Hidden Test Datasets:</strong> Unseen data with more challenging scenarios for final evaluation</li>
</ul>

<p>These datasets span multiple domains including images, videos, and multimodal data, providing a comprehensive testbed for disentangled representation learning methods.</p>

<h3>Competition Structure</h3>

<p>The competition is structured in two phases:</p>

<ul>
  <li><strong>Development Phase (January 15 - August 15, 2025):</strong> Participants develop and refine their methods with private leaderboard feedback</li>
  <li><strong>Test Phase (August 16 - October 15, 2025):</strong> Final evaluation on unseen test data with public leaderboard</li>
</ul>

<p>Winners will be announced at the DRL for Real Workshop at ICCV 2025, with opportunities for top teams to present their approaches.</p>

<p>By training and evaluating models on real-world data, our aim is to foster meaningful advancements in the DRL community and facilitate its integration into practical scenarios.</p>
