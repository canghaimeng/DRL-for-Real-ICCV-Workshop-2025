<h2>Evaluation Details for DRL for Real Competition</h2>

<p>The DRL for Real competition evaluates disentangled representation learning methods based on their performance on real-world datasets. Our evaluation framework is designed to assess both the theoretical properties of disentanglement and the practical utility of the learned representations in real-world applications.</p>

<h3>Track-Specific Evaluation Metrics</h3>

<h4>1. Single-Factor Track Evaluation</h4>

<p>In the Single-Factor Track, participants must use disentangled models. Evaluation is based on two equally weighted components:</p>

<ul>
  <li><strong>Image Generation Quality (50%)</strong>
    <ul>
      <li>Measured using FID (Fr√©chet Inception Distance)</li>
      <li>Lower FID scores indicate better quality of generated images</li>
      <li>Evaluates how well the generated images match the distribution of real images</li>
    </ul>
  </li>
  
  <li><strong>Disentanglement Quality (50%)</strong>
    <ul>
      <li>Evaluated using Bi-directional DCI, an extension of the traditional DCI metric</li>
      <li>Components of Bi-directional DCI:
        <ul>
          <li><em>Forward DCI:</em> Measures how well individual latent units capture individual factors of variation</li>
          <li><em>Backward DCI:</em> Measures how perturbations in real images affect the corresponding latent variables in the encoder</li>
        </ul>
      </li>
      <li>Higher scores indicate better disentanglement of factors in both generation and encoding</li>
    </ul>
  </li>
</ul>

<h4>2. Multi-Factor Track Evaluation</h4>

<h5>2.1 General Image Dataset</h5>

<p>This sub-track has two separate leaderboards with different evaluation metrics:</p>

<ul>
  <li><strong>Main Leaderboard (All Methods)</strong>
    <ul>
      <li>Uses LLM+VQA evaluation approach:</li>
      <li>Step 1: VQA assessment determines if models correctly generate images with attributes specified in given text</li>
      <li>Step 2: LLM evaluates attribute strength in a set of generated images</li>
      <li>Step 3: Comparison between text-specified attribute changes and LLM-detected attribute changes</li>
      <li>Final score is calculated as the ratio of intended attribute changes to total detected attribute changes</li>
      <li>Scores closer to 1 indicate better performance (minimal unintended attribute changes)</li>
    </ul>
  </li>
  
  <li><strong>Disentanglement Leaderboard (Disentangled Methods Only)</strong>
    <ul>
      <li>Uses the same metrics as the Single-Factor Track:</li>
      <li>Image Generation Quality (50%): Measured using FID</li>
      <li>Disentanglement Quality (50%): Evaluated using Bi-directional DCI</li>
    </ul>
  </li>
</ul>

<h5>2.2 Autonomous Driving Dataset</h5>

<p>This sub-track also has two separate leaderboards:</p>

<ul>
  <li><strong>Main Leaderboard (All Methods)</strong>
    <ul>
      <li>Task: Given images from one camera viewpoint, generate images for other viewpoints and time points</li>
      <li>Evaluation: Uses SSIM (Structural Similarity Index Measure) to quantify the similarity between generated images and ground truth</li>
      <li>Higher SSIM scores indicate better reconstruction quality</li>
    </ul>
  </li>
  
  <li><strong>Disentanglement Leaderboard (Disentangled Methods Only)</strong>
    <ul>
      <li>Uses the same metrics as the Single-Factor Track:</li>
      <li>Image Generation Quality (50%): Measured using FID</li>
      <li>Disentanglement Quality (50%): Evaluated using Bi-directional DCI</li>
    </ul>
  </li>
</ul>

<h3>Evaluation Process</h3>

<p>The evaluation process consists of the following steps:</p>

<ol>
  <li><strong>Submission:</strong> Participants submit their models and code for evaluation</li>
  <li><strong>Verification:</strong> Submissions are verified for compatibility with the evaluation framework</li>
  <li><strong>Execution:</strong> Models are executed on the evaluation server using the provided datasets</li>
  <li><strong>Metric Calculation:</strong> Track-specific metrics are calculated as described above:
    <ul>
      <li>Single-Factor Track: FID and Bi-directional DCI</li>
      <li>Multi-Factor Track - General Image Dataset (Main): LLM+VQA evaluation</li>
      <li>Multi-Factor Track - General Image Dataset (Disentanglement): FID and Bi-directional DCI</li>
      <li>Multi-Factor Track - Autonomous Dataset (Main): SSIM</li>
      <li>Multi-Factor Track - Autonomous Dataset (Disentanglement): FID and Bi-directional DCI</li>
    </ul>
  </li>
  <li><strong>Scoring:</strong> Final scores are calculated according to the track-specific formulas</li>
</ol>

<h3>Development vs. Test Phase Evaluation</h3>

<p><strong>Development Phase (May 31 - June 23, 2025):</strong></p>
<ul>
  <li>Models are evaluated on validation data with known factors of variation</li>
  <li>Feedback is provided on all track-specific metrics to help participants improve their models</li>
  <li>Leaderboard is private to encourage exploration of different approaches</li>
</ul>

<p><strong>Test Phase (June 23 - June 30, 2025):</strong></p>
<ul>
  <li>Models are evaluated on unseen test data with more challenging scenarios</li>
  <li>Test data includes more complex real-world examples with ambiguous factors</li>
  <li>For the Autonomous Driving Dataset, test data includes more diverse weather conditions, lighting variations, and vehicle movements</li>
  <li>For the General Image Dataset, test data includes more complex attribute combinations</li>
  <li>Leaderboard is public, showing the performance of all participants</li>
</ul>

<p>The evaluation emphasizes practical utility and real-world applicability, reflecting our workshop's focus on making disentangled representation learning truly useful for real-world applications. The most successful approaches will be those that not only achieve high disentanglement scores in controlled settings but also maintain their performance in challenging real-world scenarios.</p>